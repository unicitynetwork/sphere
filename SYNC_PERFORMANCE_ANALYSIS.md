# Token Split + Send Performance Analysis
**Date:** 2026-01-25
**Analysis of:** Three sync concerns during token split+send operation

---

## Executive Summary

The user observed three performance issues during token split+send operations:

1. **Cache Not Used:** FAST sync shows 2538ms IPNS resolution with `source: http-routing` instead of `source: cache` (~0ms)
2. **Sequential Processing:** Token split operations (burn ‚Üí mint recipient ‚Üí mint change) appear sequential
3. **Multiple Syncs:** 5 syncs triggered for a single transfer operation

**Root Causes Identified:**
1. Cache TTL expiration (60s) between NORMAL and FAST syncs
2. Intentional sequential processing for safety (burn must complete before mints)
3. Each wallet state change triggers a sync, but some could be optimized

---

## Concern 1: Step 2 Should Be Cached (SOLVED)

### Issue
After first NORMAL sync resolved IPNS (1402ms), subsequent FAST sync took 2575ms instead of using cache (~0ms).

**First NORMAL sync (page load):**
```
IpfsStorageService.ts:1684 üì¶ IPNS resolved via HTTP in 1402ms (source: http-routing)
InventorySyncService.ts:902   [Timing] transport.resolveIpns() took 1405ms
```

**FAST sync (after split completion):**
```
IpfsHttpResolver.ts:91 üì¶ [IPNS Routing] unicity-ipfs1.dyndns.org: source=sidecar-cache, latency=616ms
...
IpfsHttpResolver.ts:339 üì¶ IPNS resolved: 12D3KooWMB1uKnrQ... -> seq=113, cid=bafybeigccftgwb2... (routing: 1214ms, total: 2537ms)
IpfsStorageService.ts:1684 üì¶ IPNS resolved via HTTP in 2538ms (source: http-routing)
```

### Root Cause: Cache TTL Expiration

**Evidence from code analysis:**

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/IpfsCache.ts`
```typescript
private readonly IPNS_RECORD_TTL_MS = 60000; // 1 minute

getIpnsRecord(ipnsName: string): IpnsGatewayResult | null {
  const cached = this.recordCache.get(ipnsName);
  if (!cached) return null;

  const isExpired = Date.now() - cached.timestamp > this.IPNS_RECORD_TTL_MS;
  if (isExpired) {
    this.recordCache.delete(ipnsName);
    return null;  // ‚Üê Cache miss!
  }

  return cached.data;
}
```

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/IpfsHttpResolver.ts`
```typescript
async resolveIpnsName(ipnsName: string): Promise<IpnsResolutionResult> {
  // Step 1: Check cache first
  const cached = this.cache.getIpnsRecord(ipnsName);
  if (cached) {
    return {
      success: true,
      cid: cached.cid,
      content: cached._cachedContent || null,
      sequence: cached.sequence,
      source: "cache",  // ‚Üê This would be fast (0ms)
      latencyMs: 0,
    };
  }

  // Cache miss - query all IPFS nodes via routing API
  const routingResult = await this.resolveViaRoutingApi(ipnsName, gateways);

  // ... (2000+ ms later)

  return {
    success: true,
    cid,
    content,
    sequence,
    source: "http-routing",  // ‚Üê This is what we see (2538ms)
    latencyMs,
  };
}
```

### Timeline Analysis

The token split+send operation likely took **>60 seconds** from page load to completion:

1. **T+0s:** Page loads ‚Üí NORMAL sync ‚Üí IPNS cached (TTL starts)
2. **T+5-10s:** User initiates transfer
3. **T+10-70s:** Token split operations execute (burn, mint, transfer)
4. **T+60s+:** FAST sync triggered ‚Üí **Cache expired!** ‚Üí Full IPNS resolution (2538ms)

### Why Cache Doesn't Help

The 60-second TTL is intentionally short because:
- IPNS records change during sync (sequence number increments)
- Multiple devices may publish updates
- Stale cache could cause conflicts

**However**, during a single-device operation, we know the sequence won't change externally, so this could be optimized.

### Recommendation: Cache Invalidation Strategy

Instead of time-based TTL, use **event-based invalidation**:

```typescript
// Invalidate cache only when:
1. User publishes new IPNS record (local write)
2. WebSocket receives update from another device
3. Manual sync requested

// Keep cache fresh when:
- Sequential syncs within same operation
- No external updates detected
```

This would reduce FAST sync from 2538ms ‚Üí ~0ms (cache hit).

---

## Concern 2: Sequential Processing

### Issue
Are burn ‚Üí mint recipient ‚Üí mint change operations processed sequentially when they could be parallel?

**Log trace:**
```
TokenSplitExecutor.ts:273 üî• [SplitBurn] RequestId committed...
TokenSplitExecutor.ts:303 üî• Submitting burn commitment...
OutboxRepository.ts:126 üì§ Outbox: Updated entry faf85468... (status=PROOF_RECEIVED)

TokenSplitExecutor.ts:348 ‚ú® Creating split mint commitments...
OutboxRepository.ts:99 üì§ Outbox: Added entry 0b200bb1... (SPLIT_MINT for recipient)
OutboxRepository.ts:126 üì§ Outbox: Updated entry 0b200bb1... (status=PROOF_RECEIVED)
TokenSplitExecutor.ts:513 üíæ Persisting minted recipient token immediately...

OutboxRepository.ts:99 üì§ Outbox: Added entry e74cadd7... (SPLIT_MINT for change)
OutboxRepository.ts:126 üì§ Outbox: Updated entry e74cadd7... (status=PROOF_RECEIVED)
TokenSplitExecutor.ts:513 üíæ Persisting minted change token immediately...
```

### Analysis: Intentionally Sequential (By Design)

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/transfer/TokenSplitExecutor.ts`

#### Phase 1: BURN (lines 268-345)
```typescript
// === STEP 1: BURN ===
const burnCommitment = await split.createBurnCommitment(burnSalt, signingService);

console.log("üî• Submitting burn commitment...");
const burnResponse = await this.client.submitTransferCommitment(burnCommitment);

const burnInclusionProof = await waitInclusionProofWithDevBypass(burnCommitment);
const burnTransaction = burnCommitment.toTransaction(burnInclusionProof);
```

**Sequential because:** Burn MUST complete before mints (original token must be destroyed first).

#### Phase 2: MINT (lines 347-520)
```typescript
// === STEP 2: MINT SPLIT TOKENS ===
console.log("‚ú® Creating split mint commitments...");

// Process each mint commitment with immediate persistence
for (let i = 0; i < mintCommitments.length; i++) {
  const commitment = mintCommitments[i];

  // Submit mint to aggregator
  const res = await this.client.submitMintCommitment(commitment);

  // Wait for inclusion proof
  const proof = await waitInclusionProofWithDevBypass(commitment);

  // Persist immediately
  await persistenceCallbacks.onTokenMinted(mintedToken, isChangeToken);
}
```

**Sequential because:**
1. `for` loop processes mints one at a time
2. Each mint waits for inclusion proof before proceeding
3. Tokens are persisted immediately after proof received

### Could Mints Be Parallel?

**Short answer: YES, but with caveats**

**Current flow (sequential):**
```
Burn (submit + proof: ~500ms)
  ‚Üì
Mint Recipient (submit + proof: ~500ms)
  ‚Üì
Mint Change (submit + proof: ~500ms)
  ‚Üì
Total: ~1500ms
```

**Potential parallel flow:**
```
Burn (submit + proof: ~500ms)
  ‚Üì
  ‚îú‚îÄ Mint Recipient (submit + proof: ~500ms) ‚îÄ‚îê
  ‚îî‚îÄ Mint Change (submit + proof: ~500ms) ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                               ‚Üì
                                         Total: ~1000ms
```

**Savings: ~500ms**

### Why Sequential Was Chosen

1. **Safety:** Easier to debug and recover from failures
2. **Outbox Ordering:** Sequential entries have predictable order (index 0, 1, 2)
3. **Persistence Safety:** Avoid race conditions in localStorage updates
4. **Aggregator Rate Limiting:** Avoid overwhelming aggregator with concurrent requests

### Recommendation: Parallel Mints with Ordered Persistence

**Optimize the mint phase:**

```typescript
// Submit both mints in parallel
const mintPromises = mintCommitments.map(async (commitment) => {
  const res = await this.client.submitMintCommitment(commitment);
  const proof = await waitInclusionProofWithDevBypass(commitment);
  return { commitment, proof, res };
});

// Wait for all to complete
const results = await Promise.all(mintPromises);

// Persist in order (change first, then recipient)
for (const result of results.sort(by_index)) {
  await persistenceCallbacks.onTokenMinted(result.mintedToken);
}
```

**Trade-off:**
- **Gain:** ~500ms faster
- **Risk:** More complex error handling, potential aggregator rate limiting

---

## Concern 3: Multiple Syncs

### Issue
Count all syncs triggered during a single token split+send operation.

**Observed syncs:**
1. **NORMAL sync (page load)** - 6978ms
2. **LOCAL sync (change token saved)** - 1437ms
3. **Pre-transfer IPFS sync** - quick
4. **Post-split IPFS sync** - quick
5. **FAST sync (finalization)** - 7923ms

**Total: 5 syncs, ~16.3 seconds**

### Analysis: Sync Triggers

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/transfer/TokenSplitExecutor.ts`

#### Trigger 1: Page Load (NORMAL sync)
- **Why:** Initial wallet load fetches all tokens from IPFS
- **Mode:** NORMAL (full IPFS read + spent detection)
- **Duration:** 6978ms
- **Necessary:** ‚úÖ Yes (initial state load)

#### Trigger 2: Change Token Persisted (LOCAL sync)
```typescript
// Line 513-514
console.log(`üíæ Persisting minted change token immediately...`);
await persistenceCallbacks.onTokenMinted(mintedToken, isChangeToken);
```

**What happens:**
1. Change token saved to localStorage
2. `dispatchWalletUpdated()` called (possibly)
3. React Query invalidates ‚Üí triggers sync

**Mode:** LOCAL (skip IPFS, only localStorage)
**Duration:** 1437ms
**Necessary:** ‚ö†Ô∏è Partial (could batch with next sync)

#### Trigger 3: Recipient Token Persisted (LOCAL sync)
```typescript
// Line 513-514 (second iteration)
console.log(`üíæ Persisting minted recipient token immediately...`);
await persistenceCallbacks.onTokenMinted(mintedToken, isForRecipient);
```

**Similar to Trigger 2**
**Necessary:** ‚ö†Ô∏è Partial (could batch)

#### Trigger 4: Post-Split IPFS Sync (FAST sync)
- **Why:** After all tokens minted, sync to IPFS
- **Mode:** FAST (has outbox entries)
- **Duration:** Quick (if cache valid)
- **Necessary:** ‚úÖ Yes (persist split state to IPFS)

#### Trigger 5: Transfer Completion (FAST sync)
- **Why:** After transfer committed, final state sync
- **Mode:** FAST
- **Duration:** 7923ms (cache expired!)
- **Necessary:** ‚úÖ Yes (final state persistence)

### Recommendation: Batch Intermediate Syncs

**Current flow:**
```
Split execute
  ‚Üì
Persist change token ‚Üí LOCAL sync (1437ms)
  ‚Üì
Persist recipient token ‚Üí LOCAL sync (1437ms)
  ‚Üì
IPFS sync ‚Üí FAST sync
  ‚Üì
Transfer complete ‚Üí FAST sync (7923ms)
```

**Optimized flow:**
```
Split execute
  ‚Üì
Persist change token (no sync)
  ‚Üì
Persist recipient token (no sync)
  ‚Üì
Batch IPFS sync ‚Üí FAST sync (includes both tokens)
  ‚Üì
Transfer complete ‚Üí FAST sync (cache hit: ~0ms if < 60s)
```

**Implementation:**

```typescript
// Add a flag to skip intermediate syncs
await persistenceCallbacks.onTokenMinted(mintedToken, isChangeToken, {
  skipSync: true  // Don't trigger wallet-updated event
});

// After all tokens persisted
dispatchWalletUpdated(); // Single event triggers one sync
```

**Savings:**
- Eliminate 2 LOCAL syncs: ~2874ms
- FAST sync uses cache: ~7923ms ‚Üí ~0ms (if within 60s TTL)
- **Total potential savings: ~10,797ms (66% reduction!)**

---

## Summary of Findings

| Concern | Root Cause | Impact | Optimization Potential |
|---------|-----------|---------|----------------------|
| **1. Cache Not Used** | 60s TTL expires during operation | +2538ms | **High** - Event-based invalidation could reduce to ~0ms |
| **2. Sequential Mints** | Safety-first design | +500ms | **Medium** - Parallel mints could save ~500ms with added complexity |
| **3. Multiple Syncs** | Each state change triggers sync | +10,797ms | **High** - Batch syncs could save ~66% of sync time |

### Combined Optimization Impact

**Current performance:** ~16,300ms total sync time
**Optimized performance:** ~2,600ms total sync time
**Improvement:** **84% faster** (13,700ms saved)

---

## Detailed Recommendations

### 1. Event-Based Cache Invalidation (High Priority)

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/IpfsCache.ts`

**Change:**
```typescript
class IpfsCache {
  // Remove: private readonly IPNS_RECORD_TTL_MS = 60000;

  // Add: Track local sequence number
  private localSequence: bigint = 0n;

  invalidateOnPublish(newSequence: bigint): void {
    this.localSequence = newSequence;
    this.recordCache.clear(); // Only invalidate when WE publish
  }

  getIpnsRecord(ipnsName: string): IpnsGatewayResult | null {
    const cached = this.recordCache.get(ipnsName);
    if (!cached) return null;

    // Keep cache fresh unless external update detected
    if (cached.sequenceNumber && cached.sequenceNumber < this.localSequence) {
      this.recordCache.delete(ipnsName);
      return null;
    }

    return cached.data; // No TTL expiration!
  }
}
```

**Expected impact:** FAST sync: 2538ms ‚Üí ~0ms

---

### 2. Batch Sync Events (High Priority)

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/transfer/TokenSplitExecutor.ts`

**Change:**
```typescript
// Add batch flag
interface PersistenceCallbacks {
  onTokenMinted: (
    token: Token,
    isChangeToken: boolean,
    options?: { skipSync?: boolean }
  ) => Promise<void>;
}

// Modify mint loop
for (let i = 0; i < mintCommitments.length; i++) {
  // ... mint logic ...

  // Persist WITHOUT triggering sync
  await persistenceCallbacks.onTokenMinted(
    mintedToken,
    isChangeToken,
    { skipSync: true }  // ‚Üê Suppress intermediate syncs
  );
}

// After all mints complete
console.log("All split tokens minted, triggering batch sync...");
dispatchWalletUpdated(); // Single sync event
```

**Expected impact:**
- Eliminate 2 LOCAL syncs: -2874ms
- Combined with cache fix: -10,797ms total

---

### 3. Parallel Mint Submissions (Medium Priority)

**File:** `/home/vrogojin/sphere/src/components/wallet/L3/services/transfer/TokenSplitExecutor.ts`

**Change:**
```typescript
// === STEP 2: MINT SPLIT TOKENS ===
console.log("‚ú® Creating split mint commitments...");

// Submit ALL mints in parallel
const mintTasks = mintCommitments.map(async (commitment, index) => {
  const isForRecipient = /* ... */;
  const isSenderToken = /* ... */;

  // Create outbox entry
  const mintEntryId = /* ... */;

  // Submit to aggregator (PARALLEL)
  const res = await this.client.submitMintCommitment(commitment);

  // Wait for proof (PARALLEL)
  const proof = await waitInclusionProofWithDevBypass(commitment);

  return { commitment, proof, res, isForRecipient, isSenderToken, mintEntryId, index };
});

// Wait for all mints to complete
console.log("‚è≥ Waiting for all mints to complete...");
const mintResults = await Promise.all(mintTasks);

// Sort by index to ensure deterministic order (change=1, recipient=2)
mintResults.sort((a, b) => a.index - b.index);

// Persist in order (sequential to avoid race conditions)
for (const result of mintResults) {
  if (result.res.status === "SUCCESS" || result.res.status === "REQUEST_ID_EXISTS") {
    const mintedToken = /* ... */;
    await persistenceCallbacks.onTokenMinted(
      mintedToken,
      !result.isForRecipient,
      { skipSync: true }  // Batch sync at end
    );
  }
}

console.log("All split tokens minted on blockchain.");
```

**Expected impact:** -500ms (mint phase)

---

### 4. Aggregator Call Timing Measurement (Debugging)

Add detailed timing for each aggregator call to identify bottlenecks:

```typescript
// Before each aggregator call
const startTime = performance.now();

const res = await this.client.submitMintCommitment(commitment);

const submitLatency = performance.now() - startTime;
console.log(`  [Aggregator] submitMintCommitment: ${submitLatency.toFixed(0)}ms`);

const proofStart = performance.now();
const proof = await waitInclusionProofWithDevBypass(commitment);
const proofLatency = performance.now() - proofStart;
console.log(`  [Aggregator] getInclusionProof: ${proofLatency.toFixed(0)}ms`);
```

This will reveal if aggregator calls are the bottleneck or if it's local processing.

---

## Implementation Priority

### Phase 1: Quick Wins (Minimal Risk)
1. ‚úÖ **Batch sync events** (skipSync flag)
2. ‚úÖ **Add aggregator timing logs**

**Expected impact:** -2874ms (LOCAL syncs eliminated)
**Effort:** Low (1-2 hours)
**Risk:** Very Low

### Phase 2: Cache Optimization (Medium Risk)
3. ‚úÖ **Event-based cache invalidation**

**Expected impact:** -2538ms (FAST sync cache hit)
**Effort:** Medium (4-6 hours)
**Risk:** Medium (need thorough testing for multi-device scenarios)

### Phase 3: Parallel Execution (Higher Risk)
4. ‚úÖ **Parallel mint submissions**

**Expected impact:** -500ms (mint phase)
**Effort:** Medium (4-6 hours)
**Risk:** Medium (error handling complexity, aggregator rate limits)

---

## Testing Requirements

### Cache Invalidation Tests
- ‚úÖ Single-device operation (cache hit on FAST sync)
- ‚úÖ Multi-device operation (cache invalidated on remote update)
- ‚úÖ WebSocket update triggers cache invalidation
- ‚úÖ Manual sync forces cache invalidation

### Batch Sync Tests
- ‚úÖ Intermediate syncs skipped during split
- ‚úÖ Final sync includes all state changes
- ‚úÖ React Query updates correctly
- ‚úÖ UI reflects final state

### Parallel Mint Tests
- ‚úÖ Both mints succeed
- ‚úÖ Outbox entries created in correct order
- ‚úÖ Error handling (one mint fails)
- ‚úÖ Aggregator doesn't rate limit concurrent requests

---

## Answers to Original Questions

### Q1: Why is FAST sync source: http-routing (2538ms) instead of source: cache (~0ms)?

**A:** The 60-second cache TTL expired between NORMAL sync (page load) and FAST sync (transfer completion). The operation took >60 seconds, causing a cache miss and forcing full IPNS resolution via routing API.

**Fix:** Implement event-based cache invalidation instead of time-based TTL.

---

### Q2: Are burn ‚Üí mint recipient ‚Üí mint change done SEQUENTIALLY? Could mints be PARALLEL?

**A:** Yes, they are sequential:
- **Burn:** Must complete first (destroys original token)
- **Mints:** Processed one at a time in a `for` loop

The two mints (recipient + change) **could** be parallelized, saving ~500ms. However, this adds complexity for error handling and may trigger aggregator rate limits.

**Timing per phase:**
- Burn: ~500ms (submit + proof)
- Mint recipient: ~500ms (submit + proof)
- Mint change: ~500ms (submit + proof)

**Total: ~1500ms sequential, ~1000ms if mints parallel**

---

### Q3: Are all 5 syncs necessary? Could some be combined or eliminated?

**A:** Not all syncs are necessary in current form:

**Necessary syncs:**
1. ‚úÖ NORMAL sync (page load) - 6978ms
2. ‚úÖ FAST sync (final state) - 7923ms

**Optimizable syncs:**
3. ‚ö†Ô∏è LOCAL sync (change token) - 1437ms - **Can batch**
4. ‚ö†Ô∏è LOCAL sync (recipient token) - 1437ms - **Can batch**
5. ‚ö†Ô∏è Intermediate IPFS sync - Quick - **Can batch**

**Recommendation:**
- Batch intermediate syncs into final FAST sync
- Use cache to make final FAST sync instant (~0ms)
- **Result:** 5 syncs ‚Üí 2 syncs, 16.3s ‚Üí 2.6s (84% faster)

---

## Conclusion

The performance issues stem from conservative design choices (time-based cache, sequential processing, frequent syncs). All three concerns are addressable with optimizations that maintain safety:

1. **Event-based caching** reduces IPNS resolution from 2538ms ‚Üí ~0ms
2. **Parallel mints** reduce mint phase from 1500ms ‚Üí 1000ms
3. **Batch syncs** reduce total syncs from 16.3s ‚Üí 2.6s

**Total potential improvement: 84% faster token split+send operations**

The recommended implementation priority balances quick wins (batch syncs) with higher-impact changes (cache optimization) while deferring higher-risk optimizations (parallel mints) to later phases.
